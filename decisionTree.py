# -*- coding: utf-8 -*-
"""decisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NwU0l13bBaCpce4dVvQZYr6qVXqxGyEx
"""

# Load libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
import matplotlib.pyplot as plt

# Load dataset
pima_df = pd.read_csv('./diabetes.csv')

pima_df.head()

pima_df.describe()

pima_df.info()
shape = pima_df.shape
print(shape)

pima_df.isnull().sum()

X = pima_df.drop('Outcome', axis=1)
y = pima_df['Outcome']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

criteria = ["gini", "entropy"]
max_depths = [3, 4, 5, 6, 7, 8, 9, 10]

best_acc = 0
best_params = {}

for criterion in criteria:
    for max_depth in max_depths:
        clf = DecisionTreeClassifier(criterion=criterion, random_state=100, max_depth=max_depth, min_samples_leaf=5)
        clf.fit(X_train, y_train)

        dtree_y_pred = clf.predict(X_test)

        accuracy = metrics.accuracy_score(y_test, dtree_y_pred)

        if accuracy > best_acc:
            best_acc = accuracy
            best_params = {
                "criterion": criterion,
                "max_depth": max_depth,
            }

print("Best Accuracy:", best_acc)
print("Best Parameters:", best_params)

clf = DecisionTreeClassifier(criterion="entropy", random_state=100, max_depth=3, min_samples_leaf=5)
clf.fit(X_train, y_train)

dtree_y_pred = clf.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

dtree_y_true = y_test

accuracy = accuracy_score(dtree_y_true, dtree_y_pred)

precision = precision_score(dtree_y_true, dtree_y_pred, average='weighted')

recall = recall_score(dtree_y_true, dtree_y_pred, average='weighted')

f1_score = f1_score(dtree_y_true, dtree_y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision Score:", precision)
print("Recall Score: ", recall)
print("F1 Score: ", f1_score)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.model_selection import StratifiedKFold, cross_val_score

def plot_roc(dt_y_true, dt_probs):
    dtree_fpr, dtree_tpr, thresholds = roc_curve(dt_y_true, dt_probs)
    dtree_auc_val = auc(dtree_fpr, dtree_tpr)
    print('AUC=%0.2f'%dtree_auc_val) # -- 1.6

    # Plot ROC curve
    plt.plot(dtree_fpr, dtree_tpr, label = 'AUC=%0.2f'%dtree_auc_val, color =
'darkorange')
    plt.legend(loc = 'lower right')
    plt.plot([0,1], [0,1], 'b--')
    plt.xlim([0,1])
    plt.ylim([0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.show()
    return dtree_auc_val


dtree_probs = clf.predict_proba(X_test)[:, 1]

dtree_auc = plot_roc(dtree_y_true, dtree_probs)

max_acc, best_k_fold = 0, 0

for k in range(2, 11):
    skfold = StratifiedKFold(n_splits=k, random_state=100, shuffle=True)
    results_skfold_acc = (cross_val_score(clf, X, y, cv=skfold)).mean() * 100.0

    if results_skfold_acc > max_acc:
        max_acc = results_skfold_acc
        best_k_fold = k
    print("Accuracy: %.2f%%" % (results_skfold_acc))

best_accuracy = max_acc
best_k_fold = best_k_fold
print(best_accuracy, best_k_fold)



